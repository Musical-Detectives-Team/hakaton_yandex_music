{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da42bf38",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9e11ad",
   "metadata": {},
   "source": [
    "Импортируем библиотеки и прописываем пути, устанавливаем random_state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fad77bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import scipy\n",
    "import re\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tqdm import notebook \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "import pickle\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from nltk.stem import WordNetLemmatizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04e02a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63b778e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# мои пути \n",
    "#path_covers = 'C:\\\\Users\\\\fudou\\\\Untitled Folder 1\\\\covers.json'\n",
    "#path_meta = 'C:\\\\Users\\\\fudou\\\\Untitled Folder 1\\\\/meta.json'\n",
    "#path_lyrics = 'C:\\\\Users\\\\fudou\\\\Untitled Folder 1\\\\/lyrics.json'\n",
    "\n",
    "\n",
    "PATH_COVERS = 'covers.json'\n",
    "PATH_TRANSLETE_LYRICS = 'translated_lyrics.csv'\n",
    "PATH_META = 'meta.json'\n",
    "PATH_TRANSLETE_TITLE = 'translated_title.csv'\n",
    "\n",
    "\n",
    "\n",
    "#PATH_SAVE_UNITED_DF = 'data/preprocessing/united_df.csv'\n",
    "PATH_SAVE_TFIDFVECTORIZER = 'models/task_2_tfidf.pkl'\n",
    "\n",
    "#PATH_COVERS = 'data/raw/covers.json'\n",
    "#PATH_TRANSLETE_LYRICS = 'data/preprocessing/translated_lyrics.csv'\n",
    "#PATH_META = 'data/raw/meta.json'\n",
    "#PATH_TRANSLETE_TITLE = 'data/preprocessing/translated_title.csv'\n",
    "\n",
    "#PATH_SAVE_UNITED_DF = 'data/preprocessing/united_df.csv'\n",
    "\n",
    "#PATH_SAVE_TRAIN_FAISS = 'data/preprocessing/train_faiss.csv'\n",
    "#PATH_SAVE_TEST_FAISS = 'data/preprocessing/test_faiss.csv'\n",
    "#PATH_SAVE_VALID_FAISS = 'data/preprocessing/valid_faiss.csv'\n",
    "#PATH_SAVE_ALL_FAISS = 'data/preprocessing/all_faiss.csv'\n",
    "\n",
    "#PATH_SAVE_TFIDFVECTORIZER = 'models/task_2_tfidf.pkl'\n",
    "\n",
    "#PATH_SAVE_TRAIN_CORPUS = 'data/preprocessing/train_corpus.npz'\n",
    "#PATH_SAVE_TEST_CORPUS = 'data/preprocessing/test_corpus.npz'\n",
    "#PATH_SAVE_VALID_CORPUS = 'data/preprocessing/valid_corpus.npz'\n",
    "\n",
    "RANDOM_STATE = 54321"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329ed1af",
   "metadata": {},
   "source": [
    "## Создание препроцессинга для объединённого датасета"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b65aa2",
   "metadata": {},
   "source": [
    "В данном подразделе проведём предобработку данных и сформируем базовый датасет с которым будем работать дальше."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902fd552",
   "metadata": {},
   "source": [
    "Поскольку нам потребуется работать с текстом и проводить его лемматизацию, то сразу скачаем нужные расширения для nltk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54dc7209",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\fudou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\fudou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\fudou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3981ac3",
   "metadata": {},
   "source": [
    "Создадим лемматайзер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc4ce101",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a5947c",
   "metadata": {},
   "source": [
    "Напишем функции для очистки текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ee63be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_text(text: str) -> str:\n",
    "    new_text = re.sub(r'[^a-zA-Z0-9]', ' ', text)    \n",
    "    new_text = new_text.split()\n",
    "    new_text = new_text\n",
    "    return ' '.join(new_text).lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248bb316",
   "metadata": {},
   "source": [
    "Создадим функцию для лемматизации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3b7cf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(text: str) -> str:\n",
    "    text_list = nltk.word_tokenize(text)\n",
    "    lemm_list = []    \n",
    "    for word in text_list:\n",
    "        lemm_list.append(lemmatizer.lemmatize(word))    \n",
    "    lemm_text = \" \".join(lemm_list)        \n",
    "    return lemm_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b816dcd8",
   "metadata": {},
   "source": [
    "Создадим функцию, которая на вход будет принимать датасет, столбец с текстом и проводить лематизацию в столбец lemm_text. Поскольку операцию может быть достаточно долгой, то будем проводить её по батчам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f0defc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_df(df: pd.DataFrame, \n",
    "                 text_column: str, \n",
    "                 batch_size: int = 1000) -> pd.DataFrame:\n",
    "    df = df.copy(deep=True)\n",
    "    df = df.reset_index(drop=True)\n",
    "    column_name = f'lemm_{text_column}'\n",
    "    df[column_name] = ''\n",
    "    for i in notebook.tqdm(range(df.shape[0] // batch_size + 1)):\n",
    "        n = i * batch_size\n",
    "        df.loc[n:n + batch_size - 1, column_name] = df.loc[n:n + batch_size - 1, text_column].apply(clear_text) \n",
    "        df.loc[n:n + batch_size - 1, column_name] = df.loc[n:n + batch_size - 1, column_name].apply(lemmatize) \n",
    "    return df[column_name].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e43c94d",
   "metadata": {},
   "source": [
    "Напишем ряд функций, которые также будем использовать при объединении датасета."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a93fd7a",
   "metadata": {},
   "source": [
    "Напишем функцию для обработки текстовых данных, в том числе и для лемматизации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae2ddf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc_text(df: pd.DataFrame) -> pd.DataFrame:  \n",
    "    # заполним пропуски в translate_text меткой unknown предврительно добавим столбец с меткой\n",
    "    df['missing_text'] = df['translate_text'].isna().astype('int8')\n",
    "    df['missing_title'] = df['translate_title'].isna().astype('int8')\n",
    "    \n",
    "    df.loc[~df['translate_text'].isna(), 'lemm_text'] = lemmatize_df(df.loc[~df['translate_text'].isna()], 'translate_text')    \n",
    "    df['lemm_text'] =  df['lemm_text'].fillna('unknown') \n",
    "    \n",
    "    df.loc[~df['translate_title'].isna(), 'lemm_title'] = lemmatize_df(df.loc[~df['translate_title'].isna()], 'translate_title')    \n",
    "    df['lemm_title'] =  df['lemm_title'].fillna('unknown') \n",
    "    # удалим старые столбцы    \n",
    "    df = df.drop(['translate_title', 'translate_text'], axis = 1)     \n",
    "    \n",
    "    # на лемматизации могли полностью уйти некоторые тексты и заголовки, поэтому добавим таким соответствующую метку\n",
    "    df.loc[df['lemm_text']=='', 'missing_text'] = 1\n",
    "    df.loc[df['lemm_title']=='', 'missing_title'] = 1\n",
    "    df.loc[df['lemm_text']=='', 'lemm_text'] = 'non_lemm' \n",
    "    df.loc[df['lemm_title']=='', 'lemm_title'] = 'non_lemm'\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524cebd2",
   "metadata": {},
   "source": [
    "Создадим функцию, которая восстанавливает язык из первоначальных и полученных данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8155793d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc_language(df: pd.DataFrame) -> pd.DataFrame:  \n",
    "    # заполним пропуски в языке сначала на основе языка текста, если его нет, то по языку заголовка\n",
    "    df.loc[df['language'].isna(), 'language'] = df.loc[df['language'].isna(), 'lyric_language']\n",
    "    df.loc[df['language'].isna(), 'language'] = df.loc[df['language'].isna(), 'title_language']\n",
    "    df.loc[df['language'].isna(), 'language'] = 'unknown'\n",
    "    # удалим использованные столбцы с языками    \n",
    "    df = df.drop(['lyric_language', 'title_language'], axis = 1)    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42b6ab4",
   "metadata": {},
   "source": [
    "Напишем функцию для получения признаков из ISRC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed6f28f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_isrc(df: pd.DataFrame) -> pd.DataFrame:  \n",
    "    df = df.copy(deep = True)\n",
    "    # вытягиваем признаки из isrc делае предварительно стодбец с меткой отсутсвия данного параметра\n",
    "    df['missing_isrc'] = df['isrc'].isna().astype('int8') \n",
    "    df['isrc'] = df['isrc'].str.replace('-', '') # уберём дефисы\n",
    "    df['isrc'] = df['isrc'].str.replace(' ', '') # предусмотрим возможность убрать пробелы\n",
    "    # получим год создания записи в виде строки\n",
    "    df.loc[~df['isrc'].isna(), 'cat_year_isrc'] = (df.loc[~df['isrc'].isna(), 'isrc']\n",
    "                                                               .str[5:7].apply(lambda x: '19' +  x if int(x) > 23 \n",
    "                                                                               else '20' + x))\n",
    "    # преобразуем в числовой признак\n",
    "    df.loc[~df['cat_year_isrc'].isna(),'num_year'] = df.loc[~df['cat_year_isrc'].isna(), 'cat_year_isrc']\n",
    "    # для строк с пропусками вытянем год из dttm\n",
    "    df.loc[df['cat_year_isrc'].isna(),'num_year'] = df.loc[df['cat_year_isrc'].isna(), 'dttm'].dt.year\n",
    "    df['num_year'] = df['num_year'].astype('int')\n",
    "    \n",
    "    # извлекаем код страны\n",
    "    df['country_of_track'] = df['isrc'].str[:2]\n",
    "    # извлекаем код регистранта\n",
    "    df['reg_code'] = df['isrc'].str[3:6]\n",
    "    df[['cat_year_isrc', 'country_of_track', 'reg_code']] = df[['cat_year_isrc', \n",
    "                                                                'country_of_track', \n",
    "                                                                'reg_code']].fillna('unknown')\n",
    "    # удалим столбец isrcc    \n",
    "    df = df.drop('isrc', axis = 1)   \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e173fbdf",
   "metadata": {},
   "source": [
    "Напишем функцию для получения жанров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aacecaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc_genres(df: pd.DataFrame) -> pd.DataFrame:  \n",
    "    df = df.copy(deep = True) \n",
    "    # создадим словарь с количеством вхождений жанров, отсортируем его и возьмём топ 15 популярных жанров\n",
    "    dict_genres = dict()\n",
    "    for genres in df['genres'].values:\n",
    "        for genre in genres:\n",
    "            if genre not in dict_genres.keys():\n",
    "                dict_genres[genre] = 0\n",
    "            dict_genres[genre] += 1\n",
    "    dict_genres = sorted(dict_genres.items(), key=lambda item: item[1], reverse=True)\n",
    "    top_15_genres = [x[0] for x in dict_genres [:15]]\n",
    "\n",
    "    # закодируем их\n",
    "    for genre in top_15_genres:\n",
    "        df[f'genre_{genre}'] = df['genres'].apply(lambda x: 1 if genre in x else 0)\n",
    "\n",
    "    # обработаем треки без жанра   \n",
    "    mask = df['genres'].isin([[]])\n",
    "    df['uncnown_genre'] = mask.astype('int8')\n",
    "\n",
    "    # обработаем невошедшие треки\n",
    "    df['genre_other'] = 0\n",
    "    df.loc[~mask, 'genre_other'] = (df.loc[~mask, 'genres']\n",
    "                                          .apply(lambda x: max([1 if i not in top_15_genres else 0 for i in x])))\n",
    "\n",
    "    # сделаем группировку по крпуным жанрам\n",
    "    for group_genre in ['ROCK', 'RAP', 'POP', 'FOLK', 'RUS']:\n",
    "        column = f'genre_group_{group_genre}'\n",
    "        df[column] = 0\n",
    "        df.loc[~mask, column] = (df.loc[~mask, 'genres']\n",
    "                                          .apply(lambda x: max([1 if group_genre in i else 0 for i in x])))\n",
    "    df = df.drop('genres', axis = 1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41ebf58",
   "metadata": {},
   "source": [
    "Напишем функцию, которая на вход будет принимать пути к нашим датасетам, проводить чистку данных и возвращать объединённый датасет. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76830adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_united_df(path_covers: str, path_lyrics: str, path_title: str, path_meta: str) -> pd.DataFrame:\n",
    "    df_covers = pd.read_json(path_covers, lines=True)\n",
    "    df_lyrics = pd.read_csv(PATH_TRANSLETE_LYRICS)[['track_id', 'language', 'translate_text']]\n",
    "    df_meta = pd.read_json(path_meta, lines=True, convert_dates=['dttm'])\n",
    "    df_title = pd.read_csv(PATH_TRANSLETE_TITLE)[['track_id', 'title_language', 'translate_title']]\n",
    "        \n",
    "    # удалим дубликаты треков в df_lyric (по track_id)\n",
    "    df_lyrics = df_lyrics.rename(columns={'language': 'lyric_language'})\n",
    "    df_lyrics = df_lyrics.drop_duplicates(subset='track_id')\n",
    "    \n",
    "    # из df_meta удалим строку с пропуском и убираем треки с 0 длительностью\n",
    "    df_meta = df_meta.dropna(subset='track_id')\n",
    "    df_meta = df_meta.drop('title', axis = 1)\n",
    "    df_meta = df_meta[df_meta['duration']!=0]\n",
    "    \n",
    "    # объединяем через left, чтобы не потерять данные\n",
    "    df_union = (df_meta.merge(df_covers, on = 'track_id', how = 'left')\n",
    "                     .merge(df_lyrics, on = 'track_id', how = 'left')\n",
    "                     .merge(df_title, on = 'track_id', how = 'left'))\n",
    "    \n",
    "    df_union = preproc_text(df_union)    \n",
    "    df_union = preproc_language(df_union)\n",
    "    df_union = get_features_isrc(df_union)\n",
    "    df_union = preproc_genres(df_union)\n",
    "    \n",
    "    return df_union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05230208",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Index(...) must be called with a collection of some kind, 'track_id' was passed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_union \u001b[38;5;241m=\u001b[39m \u001b[43mget_united_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPATH_COVERS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPATH_TRANSLETE_LYRICS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPATH_TRANSLETE_TITLE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPATH_META\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m df_union\u001b[38;5;241m.\u001b[39mshape\n",
      "Cell \u001b[1;32mIn[13], line 12\u001b[0m, in \u001b[0;36mget_united_df\u001b[1;34m(path_covers, path_lyrics, path_title, path_meta)\u001b[0m\n\u001b[0;32m      9\u001b[0m df_lyrics \u001b[38;5;241m=\u001b[39m df_lyrics\u001b[38;5;241m.\u001b[39mdrop_duplicates(subset\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrack_id\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# из df_meta удалим строку с пропуском и убираем треки с 0 длительностью\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m df_meta \u001b[38;5;241m=\u001b[39m \u001b[43mdf_meta\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrack_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m df_meta \u001b[38;5;241m=\u001b[39m df_meta\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m, axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     14\u001b[0m df_meta \u001b[38;5;241m=\u001b[39m df_meta[df_meta[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mduration\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m!=\u001b[39m\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ds_practicum_env\\lib\\site-packages\\pandas\\core\\frame.py:5159\u001b[0m, in \u001b[0;36mDataFrame.dropna\u001b[1;34m(self, axis, how, thresh, subset, inplace)\u001b[0m\n\u001b[0;32m   5157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m subset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5158\u001b[0m     ax \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_axis(agg_axis)\n\u001b[1;32m-> 5159\u001b[0m     indices \u001b[38;5;241m=\u001b[39m \u001b[43max\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_indexer_for\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5160\u001b[0m     check \u001b[38;5;241m=\u001b[39m indices \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   5161\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m check\u001b[38;5;241m.\u001b[39many():\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ds_practicum_env\\lib\\site-packages\\pandas\\core\\indexes\\base.py:4960\u001b[0m, in \u001b[0;36mIndex.get_indexer_for\u001b[1;34m(self, target, **kwargs)\u001b[0m\n\u001b[0;32m   4948\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4949\u001b[0m \u001b[38;5;124;03mGuaranteed return of an indexer even when non-unique.\u001b[39;00m\n\u001b[0;32m   4950\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4957\u001b[0m \u001b[38;5;124;03m    List of indices.\u001b[39;00m\n\u001b[0;32m   4958\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4959\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_as_unique:\n\u001b[1;32m-> 4960\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_indexer(target, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   4961\u001b[0m indexer, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_indexer_non_unique(target)\n\u001b[0;32m   4962\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m indexer\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ds_practicum_env\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3147\u001b[0m, in \u001b[0;36mIndex.get_indexer\u001b[1;34m(self, target, method, limit, tolerance)\u001b[0m\n\u001b[0;32m   3142\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(_index_shared_docs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget_indexer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m%\u001b[39m _index_doc_kwargs)\n\u001b[0;32m   3143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_indexer\u001b[39m(\n\u001b[0;32m   3144\u001b[0m     \u001b[38;5;28mself\u001b[39m, target, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, tolerance\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   3145\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[0;32m   3146\u001b[0m     method \u001b[38;5;241m=\u001b[39m missing\u001b[38;5;241m.\u001b[39mclean_reindex_fill_method(method)\n\u001b[1;32m-> 3147\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[43mensure_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tolerance \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3149\u001b[0m         tolerance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_tolerance(tolerance, target)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ds_practicum_env\\lib\\site-packages\\pandas\\core\\indexes\\base.py:5917\u001b[0m, in \u001b[0;36mensure_index\u001b[1;34m(index_like, copy)\u001b[0m\n\u001b[0;32m   5914\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[0;32m   5915\u001b[0m         index_like \u001b[38;5;241m=\u001b[39m copy_func(index_like)\n\u001b[1;32m-> 5917\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mIndex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex_like\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ds_practicum_env\\lib\\site-packages\\pandas\\core\\indexes\\base.py:372\u001b[0m, in \u001b[0;36mIndex.__new__\u001b[1;34m(cls, data, dtype, copy, name, tupleize_cols, **kwargs)\u001b[0m\n\u001b[0;32m    369\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_simple_new(subarr, name)\n\u001b[0;32m    371\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m is_scalar(data):\n\u001b[1;32m--> 372\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_scalar_data_error(data)\n\u001b[0;32m    373\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(data, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__array__\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    374\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Index(np\u001b[38;5;241m.\u001b[39masarray(data), dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy, name\u001b[38;5;241m=\u001b[39mname, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mTypeError\u001b[0m: Index(...) must be called with a collection of some kind, 'track_id' was passed"
     ]
    }
   ],
   "source": [
    "df_union = get_united_df(PATH_COVERS, PATH_TRANSLETE_LYRICS, PATH_TRANSLETE_TITLE, PATH_META)\n",
    "df_union.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561bad4d",
   "metadata": {},
   "source": [
    "Выведем первые 5 строк датасета и информацию по нему"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a14593",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_union.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bc6fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_union.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a34f2b8",
   "metadata": {},
   "source": [
    "У нас формирован первичный датасет для дальнейших задач, в нём есть пропуски, но одни из них не критичны для решения 1 задачи, другие для второй. Поэтому дальнейшую обработку будем вести отдельно для каждой задачи. \n",
    "\n",
    "Сохраним получившийся датасет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be66cc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_union.to_csv(PATH_SAVE_UNITED_DF, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccd7e91-7223-4825-ba10-fb26408fee46",
   "metadata": {},
   "source": [
    "**Вывод**\n",
    "Все тексты были переведены на английский язык с помощью Yandex Translator, что значительно упростит задачу очистки текстов и их векторизацию\n",
    "Почти для всех тестов был определён язык на котором они написаны, а это может стать сильным признаком для модели машинного обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d809ad6-86fd-4eba-bd78-50a95f87ff9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "98a9120c",
   "metadata": {},
   "source": [
    "## Обработка датасета для задачи нахождения групп каверов / оригиналов (2 задача)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5acd923",
   "metadata": {},
   "source": [
    "Данный подраздел будет посвящён созданию датасета для решения второй задачи, а также векторизации текста для данной задачи. Чтобы решить эту задачу необходимо наличие столбца original_track_id, чтобы понять какие треки являются каверами, для каких треков. А также наличие текста как такового."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f82cab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_task_2 = df_union.dropna(subset='original_track_id')\n",
    "df_task_2 = df_task_2[df_task_2['missing_text'] == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99de0ada",
   "metadata": {},
   "source": [
    "Для решения данной задачи нам не нужна дата заливки в базу и track_remake_type. Первый столбец удалим, а второй оставим, поскольку он нам может помочь оценивать модель и сами данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f8ca8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_task_2 = df_task_2.drop('dttm', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49967be9",
   "metadata": {},
   "source": [
    "выведем информацию о датасете и первые 5 строк"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720fbc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_task_2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320c960b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_task_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34efc448",
   "metadata": {},
   "source": [
    "У нас нет пропусков, однако индексы идут в разнобой, поэтому сбросим их"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7211b158",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_task_2 = df_task_2.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee43fba",
   "metadata": {},
   "source": [
    "Теперь нам нужно создать столбец в котором будут указаны все каверы / оригиналы для конкретного original_track_id и посчитать их количество"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92797b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_covers_column(df: pd.DataFrame, count_cover=True) -> pd.DataFrame:\n",
    "    df_new = df.copy(deep=True)\n",
    "    df_group = df.groupby('original_track_id', as_index= False ).agg({'track_id': ' '. join})\n",
    "    df_group = df_group.rename(columns = {'track_id': 'cover_list'})\n",
    "    df_group['cover_list'] = df_group['cover_list'].str.split()\n",
    "    \n",
    "    # присоединим список с датасетом и удалим из списка сам трек\n",
    "    df_new = df_new.merge(df_group, on = 'original_track_id', how='inner')\n",
    "    df_new['cover_list'] = df_new.apply(lambda x: list(set(x['cover_list']) ^ {x['track_id']}), axis=1)\n",
    "    \n",
    "    if count_cover:\n",
    "        df_new['cover_count'] = df_new['cover_list'].apply(lambda x: len(x))\n",
    "\n",
    "    #преобразуем в строку, поскольку планируем сохранять датасет в csv\n",
    "    df_new['cover_list'] = df_new['cover_list'].apply(lambda x: ' '.join(x))\n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa1982f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_task_2 = make_covers_column(df_task_2)\n",
    "df_task_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9b16a0",
   "metadata": {},
   "source": [
    "Чтобы корректно разделить выборку, добавим столбец для стратификации, для этого выделим группы по количеству каверов у песни. Для этого построим гистограмму"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf3bee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_task_2['cover_count'].hist(bins=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c91cb2",
   "metadata": {},
   "source": [
    "Выделим следующие группы:\n",
    "* количество каверов равное 0\n",
    "* количество каверов от 1 до 10\n",
    "* количество каверов от 10 до 20\n",
    "* количество каверов больше 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c0b740",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_task_2.loc[df_task_2['cover_count']==0, 'group_count_cover'] = 'group_1'\n",
    "df_task_2.loc[(df_task_2['cover_count']>0) &\n",
    "             (df_task_2['cover_count']<=10), 'group_count_cover'] = 'group_2'\n",
    "df_task_2.loc[(df_task_2['cover_count']>10) &\n",
    "             (df_task_2['cover_count']<=20), 'group_count_cover'] = 'group_3'\n",
    "df_task_2.loc[df_task_2['cover_count']>20, 'group_count_cover'] = 'group_4'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7964975e",
   "metadata": {},
   "source": [
    "Удалим ненужный столбец cover_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03476b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_task_2 = df_task_2.drop('cover_count', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e64d3b1",
   "metadata": {},
   "source": [
    "Разделим датасет на тренировочную, тестовую и валидационную выборки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c684e75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df_task_2,\n",
    "                                     random_state=RANDOM_STATE,\n",
    "                                     test_size=0.3,\n",
    "                                     stratify=df_task_2['group_count_cover'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621b539d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a532d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test, df_valid = train_test_split(df_test,\n",
    "                                     random_state=RANDOM_STATE,\n",
    "                                     test_size=0.5,\n",
    "                                     stratify=df_test['group_count_cover'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c223d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.shape, df_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb512cd4",
   "metadata": {},
   "source": [
    "Теперь удалим созданные столбцы и пересоздадим их заново (тренировочная выборка не должна ничего знать о тестовой и валидационной, а для тестовой и валидационной выборки нас будут интересовать количество каверов из трейна). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7a8d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop(['cover_list', 'group_count_cover'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301a8be2",
   "metadata": {},
   "source": [
    "Для тестовой и валиадционной выборки удалим только столбец с группами, и найдём пересечение со списком track_id из трейновой выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdee7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test.drop(['group_count_cover'], axis=1)\n",
    "df_valid = df_valid.drop(['group_count_cover'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c4f0f5",
   "metadata": {},
   "source": [
    "Получим список каверов для трэйна "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4bdfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = make_covers_column(df_train, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eecbfbb",
   "metadata": {},
   "source": [
    "Получим множество track_id в трэйновом датасете"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba29a9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_train_track = set(df_train['track_id'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54afc70",
   "metadata": {},
   "source": [
    "Найдём пересечения данного множества со списками значений в cover_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e49e193",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['cover_list'] = df_test.apply(lambda x: ' '.join(set(x['cover_list'].split()) & set_train_track), axis=1)\n",
    "df_valid['cover_list'] = df_valid.apply(lambda x: ' '.join(set(x['cover_list'].split()) & set_train_track), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fabefee",
   "metadata": {},
   "source": [
    "Сохраним данные датасеты, они потребуется на обучении модели (поскольку там будем подбирать параметры модели)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d824d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv(PATH_SAVE_TRAIN_FAISS, index=False)\n",
    "df_test.to_csv(PATH_SAVE_TEST_FAISS, index=False)\n",
    "df_valid.to_csv(PATH_SAVE_VALID_FAISS, index=False)\n",
    "df_task_2.to_csv(PATH_SAVE_ALL_FAISS, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bb94b0",
   "metadata": {},
   "source": [
    "## Создание корпусов текста"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e33f043",
   "metadata": {},
   "source": [
    "Для обработки текста мы планируем воспользоваться TfIdf, на предыдущих этапах мы уже получили лематизированный текст."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5dd01c6",
   "metadata": {},
   "source": [
    "Создадим корпусы для обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd815693",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_train = df_train['lemm_text']\n",
    "corpus_test = df_test['lemm_text']\n",
    "corpus_valid = df_valid['lemm_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2014ee71",
   "metadata": {},
   "source": [
    "Загрузим стоп слова для английского языка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace026e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "stopwords = nltk_stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79fe281",
   "metadata": {},
   "source": [
    "Обучим TfIdf и векторизуем наши корпуса. Пока будем передавать только стоп слова, но на этапе моделинга, попробуем подобрать оптимальные параметры для векторизтора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63dad6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = TfidfVectorizer(stop_words=stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034b0ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "corpus_train = tf_idf.fit_transform(corpus_train)\n",
    "corpus_test = tf_idf.transform(corpus_test)\n",
    "corpus_valid = tf_idf.transform(corpus_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b75df82",
   "metadata": {},
   "source": [
    "Выведем размеры получившихся корпусов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c575e80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_train.shape, corpus_test.shape, corpus_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130897d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(corpus_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184e0534",
   "metadata": {},
   "source": [
    "Сохраним обученный TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c386d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(tf_idf, open(PATH_SAVE_TFIDFVECTORIZER, \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90a09f4",
   "metadata": {},
   "source": [
    "Сохраним получившиеся корпуса. Поскольку это разреженные матрицы,  тонапишем функцию, которая будт их сохранять."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cb3cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_sparse_csr(filename: str, array: scipy.sparse):\n",
    "    np.savez(filename, data=array.data, indices=array.indices,\n",
    "             indptr=array.indptr, shape=array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726ec6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_sparse_csr(PATH_SAVE_TRAIN_CORPUS, corpus_train)\n",
    "save_sparse_csr(PATH_SAVE_TEST_CORPUS, corpus_test)\n",
    "save_sparse_csr(PATH_SAVE_VALID_CORPUS, corpus_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd312f22",
   "metadata": {},
   "source": [
    "## Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b459541",
   "metadata": {},
   "source": [
    "В данном разделе был проведён препроцессинг данных в ходе которого:\n",
    "\n",
    "1. Сформирован датасет для задачи кластеризации:\n",
    "    * Использовались в основном сырые данные, кроме датасета с текстами, поскольку они дополнительно отдельно переводились на английский язык. Поскольку данная задача веьма трудозатратная, её выделили в отдельный ноутбук;\n",
    "    * Данные были очищены, убраны все пропуски, поскольку для обучения модели кластеризации необходимо знать конкретные каверы или оригиналы для треков;\n",
    "    * Добавлен целевой столбец, содержащий список каверов (оригиналов) для трека;\n",
    "    * Данные были разделены на тренировочную, тестовую и валидационные выборки для стратификации был создан временный столбец с группировкой по количеству каверов; \n",
    "        \n",
    "2. Проведена векторизация текстов:\n",
    "    * Текст был очищен от лишних символов и проведена лематизация при помощи библиотекии nltk;\n",
    "    * Обработанные датасеты были сохранены для дальнейшего использования на этапе моделинга;\n",
    "    * Была проведена векторизация текстов при помощи TfidfVectorizer, на данном этапе в качестве параметров передавались только стоп слова.\n",
    "    * Обученный TfidfVectorizer и векторизованные корпуса были так же сохранены для дальнейшего использования на этапе моделинга."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ceaca14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
